---
title: "Data Science HW-5"
author: "Kasturi Bhamidipati"
date: "2022-11-14"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(patchwork)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 9, 
  fig.height = 7,
  out.width = "95%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r, message=FALSE, warning=FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

# Problem 2

## Importing the data

First we want to import the data. 

```{r import data, message=FALSE, warning=FALSE}
homicide_raw = 
  read_csv(file = "./data/homicide-data.csv")
```

## Summary of the dataset 

- The `homicide` dataset has `r nrow(homicide_raw)` rows and `r ncol(homicide_raw)` columns. 
- It has the following variables: `r colnames(homicide_raw)`

## Creating `city_state` variable and summarizing

```{r city_state}
homicide_data = 
  homicide_raw%>%
  janitor::clean_names()%>%
  mutate(
    city_state = str_c(city,state, sep = ","),
    status = 
      case_when(
        disposition == "Closed without arrest" ~ "unsolved", 
        disposition == "Open/No arrest" ~ "unsolved", 
        disposition == "Closed by arrest" ~ "solved"
      ))%>%
  group_by(city_state)%>%
  summarise(
    total= n(), 
    unsolved = sum(status=="unsolved")
  )
```

## `prop.test` for Baltimore and saving output as R object

```{r prop test baltimore}
prop.test(
  homicide_data %>% filter(city_state == "Baltimore,MD") %>% 
    pull(unsolved),
  homicide_data %>% filter(city_state == "Baltimore,MD") %>% 
    pull(total)) %>% 
  broom::tidy()%>% 
  saveRDS(., "./data/Baltimore_prop_test.rds")
```

## `prop.test` for each city

```{r prop test all cities}
homicide_data_pt = 
  homicide_data%>%
  mutate(
    prop_df =map2(.x = unsolved, .y = total, ~prop.test(x = .x, n = .y)),
    tidied =map(.x = prop_df, ~broom::tidy(.x))
    ) %>% 
  select(-prop_df) %>% 
  unnest(tidied) %>% 
  select(city_state, estimate, conf.low,conf.high)

homicide_data_pt
```

## Plot for estimates and CIs for each city 

```{r plot, dpi=500}
homicide_plot = 
  homicide_data_pt%>% 
  ggplot(aes(x = fct_reorder(city_state, estimate), y = estimate))+ 
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  labs(title = "Estimated proportions and 95% CIs of unsolved homicides, by city, in the US",
       x = "City (City, State)",
       y = "Estimated proportion and 95% CI")

homicide_plot

ggsave(path = "results", "homicide_plot.pdf")

```

Something that I notice here is that there is a datapoint for Tulsa, AL, whoch seems extremely skewed. When I searched this up, it seems that Tulsa does not exist in AL, indicating that there migth have been some error in data collection. 

# Problem 3 

## Setting up my simulation 

First I want to set up a simulation with the following design elements: n = 30, σ = 5 and μ = 0.

```{r simulation setup}

initial_sim = function(n = 30, sigma = 5, mu = 0) {
  x = rnorm(n, mean = mu, sd = sigma)
  t_test = t.test(x, conf.int = 0.95)%>%
    broom::tidy()%>%
    select(estimate,p.value)
  
    t_test
}
```

## Generating 5000 datasets 

Now we want to generate 5000 datasets from the model. 

```{r 5000 sims}
# Now we will repeat this process 5000 times 

simulations = vector("list", 1000)
for(i in 1:1000) {
  simulations[[i]]= initial_sim()
}

simulations = simulations %>%
  bind_rows()
```

## Repeating the simulation for any μ

Now we want to repeat the above for μ={1,2,3,4,5,6}

```{r any mu and power}
sim_mu = function(set){
  mu_output = vector("list", 1000)
  for (i in 1:1000) {
    mu_output[[i]] = initial_sim(mu = set)
  }
  power_output = 
    mu_output%>%
    bind_rows()
  
  power_output
}

# For power of the test 

simulation_any_mu = 
  tibble(
    mu_vals = c(0,1,2,3,4,5,6), 
    estimate_table= map(mu_vals, sim_mu)
  )%>% 
  unnest(estimate_table)%>%
  mutate(
    reject_status = ifelse(p.value < 0.05, TRUE, FALSE)
    )
```

## Plot for power vs. true value of μ

```{r plotting power, dpi= 300}
power_plot = 
  simulation_any_mu%>%
  group_by(mu_vals)%>%
  summarise(power= sum(reject_status)/1000)%>%
  ggplot(aes(x = mu_vals, y= power))+ 
  geom_point(aes(color = mu_vals), size = 2) +
  geom_line(alpha = 0.3) +
  labs(title = "Power vs. True Value of μ",
       x = "True value of μ", 
       y = "Power of the test")
power_plot
```

### Association between effect size and power

From the plot, it is evident that power increases as μ increases. Therefore, we can conclude that as the effect size, which is the true value of μ increases, the power also increases. 

## Plot for average estimate of μ vs. true value of μ

```{r average vs true and overlay, dpi=300}

all_estimates = 
  simulation_any_mu%>%
  group_by(mu_vals)%>% 
  summarise(all_samp_mean = mean(estimate))

reject_estimates =
  simulation_any_mu%>%
  group_by(mu_vals)%>% 
  filter(reject_status==TRUE)%>%
  summarise(reject_samp_mean = mean(estimate))

combined_df= 
  full_join(all_estimates, reject_estimates, by = "mu_vals")%>%
  pivot_longer(
    all_samp_mean:reject_samp_mean, 
    names_to = "samples",
    values_to = "avg_estimates"
  )

all_plot = 
  all_estimates%>%
  ggplot(aes(x=mu_vals, y=all_samp_mean))+
  geom_point(alpha = 0.5)+
  geom_line()+
  labs(title = "All Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")
all_plot

reject_plot = 
  reject_estimates%>%
  ggplot(aes(x=mu_vals, y=reject_samp_mean))+
  geom_point(alpha = 0.5)+
  geom_line()+
  labs(title = "Rejected-Null Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")

 reject_plot
 
combined_plot = 
  combined_df%>%
  ggplot(aes(x=mu_vals, y=avg_estimates, group=samples))+
  geom_point(aes(color = samples), alpha = 0.5)+
  geom_line(aes(color = samples))+
  labs(title = "True Value of μ vs.Average Estimate of μ^ in All Sample and Rejected-Null Sample",
         x = "True population mean (μ)",
         y = "The average estimate of μ^")

combined_plot
```

- From the plot above, we can observe that the sample average of μ̂ across tests for which the null is rejected is approximately equal to the true value of μ, when μ lies between 0 and 4. It is evident that  μ̂ >μ in these cases. 
- When μ => 4, then we see that μ̂ approximates μ. 
- This is because as effect size increases, the power also increases. 

